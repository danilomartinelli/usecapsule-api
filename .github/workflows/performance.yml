name: Performance Tests

on:
  schedule:
    - cron: '0 2 * * 1' # Weekly on Monday at 2 AM UTC
  workflow_dispatch:
    inputs:
      run_soak_test:
        description: 'Run soak test (long duration)'
        required: false
        default: false
        type: boolean
      base_url:
        description: 'Base URL for testing'
        required: false
        default: 'http://localhost:3000'
        type: string

env:
  NODE_VERSION: '24'
  BASE_URL: ${{ github.event.inputs.base_url || 'http://localhost:3000' }}
  RUN_SOAK_TEST: ${{ github.event.inputs.run_soak_test || 'false' }}

jobs:
  setup:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      rabbitmq:
        image: rabbitmq:3.13-management
        env:
          RABBITMQ_DEFAULT_USER: perf
          RABBITMQ_DEFAULT_PASS: perf
        ports:
          - 5672:5672
          - 15672:15672
        options: >-
          --health-cmd "rabbitmq-diagnostics -q ping"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5

    outputs:
      api_gateway_ready: ${{ steps.health_check.outputs.ready }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          echo "Installing dependencies for performance testing..."
          npm ci --legacy-peer-deps || {
            echo "❌ Dependency installation failed"
            echo "Node.js version: $(node --version)"
            echo "NPM version: $(npm --version)"
            exit 1
          }
          echo "✅ Dependencies installed successfully"

      - name: Build applications
        run: npm run build:all

      - name: Start API Gateway
        run: |
          npm run serve:gateway &
          echo "API_GATEWAY_PID=$!" >> $GITHUB_ENV
        env:
          RABBITMQ_URL: 'amqp://perf:perf@localhost:5672'

      - name: Health check
        id: health_check
        run: |
          echo "Waiting for API Gateway to be ready..."
          timeout 180s bash -c '
            while ! curl -s -f ${{ env.BASE_URL }}/health/ready >/dev/null 2>&1; do
              echo "API Gateway not ready yet, waiting..."
              sleep 5
            done
          '
          echo "✅ API Gateway is ready for performance testing"
          # Verify the health endpoint
          curl -s ${{ env.BASE_URL }}/health/ready | jq . || echo "Health endpoint response not JSON"
          echo "ready=true" >> $GITHUB_OUTPUT

      - name: Keep API Gateway running
        run: |
          # Keep the API Gateway running for performance tests
          sleep 5
          echo "API Gateway is ready for performance testing"

  performance-tests:
    name: Run Performance Tests
    runs-on: ubuntu-latest
    needs: setup
    timeout-minutes: 60
    if: needs.setup.outputs.api_gateway_ready == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install k6
        run: |
          sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Verify k6 installation
        run: k6 version

      - name: Wait for API Gateway
        run: |
          echo "Verifying API Gateway is accessible..."
          timeout 120s bash -c '
            while ! curl -s -f ${{ env.BASE_URL }}/health/ready >/dev/null 2>&1; do
              echo "Waiting for API Gateway..."
              sleep 5
            done
          '
          echo "✅ API Gateway is ready for performance tests"

      - name: Run Performance Test Suite
        run: |
          chmod +x test/performance/run-performance-tests.sh
          test/performance/run-performance-tests.sh
        env:
          BASE_URL: ${{ env.BASE_URL }}
          RUN_SOAK_TEST: ${{ env.RUN_SOAK_TEST }}

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results
          path: test/performance/results/
          retention-days: 30

      - name: Parse performance results
        id: parse_results
        if: always()
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "test/performance/results/performance_report_*.md" ]; then
            cat test/performance/results/performance_report_*.md >> $GITHUB_STEP_SUMMARY
          else
            echo "No performance report found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check performance thresholds
        run: |
          # Check if any thresholds were violated
          threshold_violations=false

          if find test/performance/results/ -name "*_summary.txt" -exec grep -l "✗" {} \; | head -1 > /dev/null; then
            echo "❌ Performance thresholds were violated!"
            find test/performance/results/ -name "*_summary.txt" -exec grep "✗" {} \;
            threshold_violations=true
          else
            echo "✅ All performance thresholds were met"
          fi

          # Set output for potential notification
          echo "THRESHOLD_VIOLATIONS=$threshold_violations" >> $GITHUB_ENV

      - name: Comment on PR if thresholds violated
        if: github.event_name == 'pull_request' && env.THRESHOLD_VIOLATIONS == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '⚠️ **Performance Alert**: Some performance thresholds were violated in this PR. Please check the performance test results in the workflow artifacts.'
            })

  benchmark-comparison:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    needs: performance-tests
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download performance results
        uses: actions/download-artifact@v4
        with:
          name: performance-results
          path: current-results/

      - name: Download previous benchmark (if exists)
        uses: actions/download-artifact@v4
        with:
          name: performance-benchmark
          path: benchmark/
        continue-on-error: true

      - name: Compare performance metrics
        run: |
          echo "## Performance Benchmark Comparison" >> comparison.md
          echo "" >> comparison.md

          if [ -d "benchmark" ] && [ -d "current-results" ]; then
            echo "Comparing current results with previous benchmark..." >> comparison.md
            echo "" >> comparison.md
            
            # Simple comparison logic - can be enhanced
            current_report=$(find current-results -name "performance_report_*.md" | head -1)
            benchmark_report=$(find benchmark -name "performance_report_*.md" | head -1)
            
            if [ -f "$current_report" ] && [ -f "$benchmark_report" ]; then
              echo "### Current vs Benchmark" >> comparison.md
              echo "Current results available: ✅" >> comparison.md
              echo "Benchmark available: ✅" >> comparison.md
            else
              echo "Unable to compare - missing reports" >> comparison.md
            fi
          else
            echo "No previous benchmark available. Current results will become the new benchmark." >> comparison.md
          fi

          cat comparison.md >> $GITHUB_STEP_SUMMARY

      - name: Save current results as new benchmark
        if: github.ref == 'refs/heads/main' && github.event_name == 'schedule'
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmark
          path: current-results/
          retention-days: 90

  notify-on-failure:
    name: Notify on Performance Issues
    runs-on: ubuntu-latest
    needs: [performance-tests, benchmark-comparison]
    if: failure() && github.ref == 'refs/heads/main'

    steps:
      - name: Send notification
        run: |
          echo "Performance tests failed on main branch"
          echo "This could indicate a performance regression"
          # Add Slack/email notification logic here if needed

  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [performance-tests, benchmark-comparison]
    if: always()

    steps:
      - name: Cleanup old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            // Clean up old performance artifacts to manage storage
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100
            });

            const performanceArtifacts = artifacts.data.artifacts.filter(artifact => 
              artifact.name.includes('performance-results') && 
              new Date(artifact.created_at) < new Date(Date.now() - 30 * 24 * 60 * 60 * 1000) // 30 days old
            );

            for (const artifact of performanceArtifacts.slice(10)) { // Keep latest 10
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id
              });
            }
